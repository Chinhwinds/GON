# Fine-tuning Mistral 7B cho Táº¡o CÃ¢u Chuyá»‡n MÃ´i TrÆ°á»ng

## ğŸ“‹ Tá»•ng quan

HÆ°á»›ng dáº«n fine-tuning mÃ´ hÃ¬nh Mistral 7B Ä‘á»ƒ táº¡o cÃ¢u chuyá»‡n vá» báº£o vá»‡ mÃ´i trÆ°á»ng dá»±a trÃªn tháº» bÃ i trong game GON.

## ğŸ¯ Má»¥c tiÃªu

- Táº¡o cÃ¢u chuyá»‡n sinh Ä‘á»™ng, tá»± nhiÃªn tá»« thÃ´ng tin tháº» bÃ i
- Duy trÃ¬ phong cÃ¡ch ká»ƒ chuyá»‡n nháº¥t quÃ¡n
- TÃ­ch há»£p hiá»‡u á»©ng game vÃ o cÃ¢u chuyá»‡n má»™t cÃ¡ch mÆ°á»£t mÃ 
- Táº¡o ra ná»™i dung giÃ¡o dá»¥c vá» mÃ´i trÆ°á»ng

## ğŸ“Š Dataset

### Format dá»¯ liá»‡u

- **File**: `fine_tuning_data.jsonl`
- **Format**: JSONL (JSON Lines)
- **Cáº¥u trÃºc**: Chat format vá»›i system, user, assistant messages

### Thá»‘ng kÃª dataset

- **Tá»•ng sá»‘ máº«u**: 24
- **Loáº¡i tháº»**: Báº£o Vá»‡ (8), Cá»™ng Äá»“ng (8), ThiÃªn Tai (8)
- **Äá»™ dÃ i cÃ¢u chuyá»‡n**: 1-2 cÃ¢u ngáº¯n gá»n

## ğŸ› ï¸ Cáº¥u hÃ¬nh Fine-tuning

### 1. YÃªu cáº§u há»‡ thá»‘ng

```bash
# GPU requirements
- VRAM: 24GB+ (A100, V100, RTX 4090)
- RAM: 64GB+
- Storage: 100GB+ free space
```

### 2. CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

```bash
# Táº¡o virtual environment
python -m venv mistral_finetune
source mistral_finetune/bin/activate  # Linux/Mac
# hoáº·c
mistral_finetune\Scripts\activate     # Windows

# CÃ i Ä‘áº·t dependencies
pip install torch transformers datasets accelerate
pip install peft bitsandbytes wandb
pip install trl sentencepiece
```

### 3. Script Fine-tuning

Táº¡o file `train_mistral.py`:

```python
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging
)
from peft import LoraConfig, PeftModel
from trl import SFTTrainer
import os

# Cáº¥u hÃ¬nh quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False,
)

# Load model vÃ  tokenizer
model_name = "mistralai/Mistral-7B-v0.1"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    torch_dtype=torch.float16,
)
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# Cáº¥u hÃ¬nh LoRA
peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ]
)

# Cáº¥u hÃ¬nh training
training_args = TrainingArguments(
    output_dir="./mistral-environmental-stories",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    gradient_checkpointing=True,
    optim="paged_adamw_32bit",
    logging_steps=10,
    save_strategy="epoch",
    learning_rate=2e-4,
    fp16=True,
    max_grad_norm=0.3,
    max_steps=-1,
    warmup_ratio=0.03,
    group_by_length=True,
    lr_scheduler_type="constant",
    report_to="wandb"
)

# Trainer
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field="text",
    max_seq_length=2048,
    tokenizer=tokenizer,
    args=training_args,
    packing=False,
)

# Báº¯t Ä‘áº§u training
trainer.train()

# LÆ°u model
trainer.save_model()
```

### 4. Cáº¥u hÃ¬nh Dataset

Táº¡o file `dataset_config.py`:

```python
from datasets import Dataset
import json

def load_training_data():
    data = []
    with open('fine_tuning_data.jsonl', 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))

    # Convert to text format for training
    texts = []
    for item in data:
        messages = item['messages']
        text = ""
        for msg in messages:
            if msg['role'] == 'system':
                text += f"<|system|>\n{msg['content']}\n<|end|>\n"
            elif msg['role'] == 'user':
                text += f"<|user|>\n{msg['content']}\n<|end|>\n"
            elif msg['role'] == 'assistant':
                text += f"<|assistant|>\n{msg['content']}\n<|end|>\n"
        texts.append(text)

    return Dataset.from_dict({"text": texts})

# Load dataset
dataset = load_training_data()
```

## ğŸš€ Cháº¡y Fine-tuning

```bash
# Cháº¡y training
python train_mistral.py

# Monitor vá»›i wandb
wandb login
```

## ğŸ“ˆ Hyperparameters Ä‘Æ°á»£c khuyáº¿n nghá»‹

| Parameter     | Value | MÃ´ táº£                                |
| ------------- | ----- | ------------------------------------ |
| Learning Rate | 2e-4  | Tá»‘c Ä‘á»™ há»c phÃ¹ há»£p cho LoRA          |
| Batch Size    | 4     | PhÃ¹ há»£p vá»›i VRAM 24GB                |
| Epochs        | 3     | Äá»§ Ä‘á»ƒ há»c pattern mÃ  khÃ´ng overfit   |
| LoRA Rank     | 64    | CÃ¢n báº±ng giá»¯a hiá»‡u quáº£ vÃ  cháº¥t lÆ°á»£ng |
| LoRA Alpha    | 16    | Scaling factor cho LoRA              |
| Warmup Ratio  | 0.03  | 3% steps cho warmup                  |

## ğŸ”§ Tá»‘i Æ°u hÃ³a

### 1. Data Augmentation

```python
# Táº¡o thÃªm dá»¯ liá»‡u báº±ng cÃ¡ch biáº¿n thá»ƒ
def augment_data(original_data):
    augmented = []
    for item in original_data:
        # ThÃªm biáº¿n thá»ƒ vá»›i tá»« Ä‘á»“ng nghÄ©a
        # Thay Ä‘á»•i cáº¥u trÃºc cÃ¢u
        # ThÃªm chi tiáº¿t mÃ´i trÆ°á»ng
        pass
    return augmented
```

### 2. Prompt Engineering

```python
# Cáº£i thiá»‡n system prompt
system_prompt = """
Báº¡n lÃ  má»™t AI chuyÃªn táº¡o cÃ¢u chuyá»‡n vá» báº£o vá»‡ mÃ´i trÆ°á»ng dá»±a trÃªn tháº» bÃ i.
HÃ£y táº¡o cÃ¢u chuyá»‡n:
- Ngáº¯n gá»n (1-2 cÃ¢u)
- Sinh Ä‘á»™ng vá»›i chi tiáº¿t cá»¥ thá»ƒ
- CÃ³ Ã½ nghÄ©a giÃ¡o dá»¥c vá» mÃ´i trÆ°á»ng
- TÃ­ch há»£p hiá»‡u á»©ng game má»™t cÃ¡ch tá»± nhiÃªn
- Sá»­ dá»¥ng ngÃ´n ngá»¯ phÃ¹ há»£p vá»›i tráº» em
"""
```

## ğŸ“Š ÄÃ¡nh giÃ¡ Model

### 1. Metrics

- **Perplexity**: Äá»™ phá»©c táº¡p cá»§a model
- **BLEU Score**: Äá»™ chÃ­nh xÃ¡c cá»§a output
- **Human Evaluation**: ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng cÃ¢u chuyá»‡n

### 2. Test Cases

```python
test_cases = [
    "TÃªn tháº»: Trá»“ng Rá»«ng Äáº§u Nguá»“n. Loáº¡i: Báº£o Vá»‡. Hiá»‡u á»©ng: +2 Ä‘iá»ƒm cho Rá»«ng.",
    "TÃªn tháº»: ChÃ¡y rá»«ng. Loáº¡i: ThiÃªn Tai. Hiá»‡u á»©ng: -2 Ä‘iá»ƒm Rá»«ng.",
    # ThÃªm test cases khÃ¡c
]
```

## ğŸ”„ Integration vá»›i App

### 1. Load Fine-tuned Model

```python
def load_fine_tuned_model():
    model = AutoModelForCausalLM.from_pretrained(
        "./mistral-environmental-stories",
        torch_dtype=torch.float16,
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained("./mistral-environmental-stories")
    return model, tokenizer
```

### 2. Generate Story

```python
def generate_story(card_info, model, tokenizer):
    prompt = f"<|system|>\nBáº¡n lÃ  má»™t AI chuyÃªn táº¡o cÃ¢u chuyá»‡n vá» báº£o vá»‡ mÃ´i trÆ°á»ng.\n<|end|>\n<|user|>\n{card_info}\n<|end|>\n<|assistant|>\n"

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("<|assistant|>\n")[-1].strip()
```

## ğŸ“ Checklist Fine-tuning

- [ ] Chuáº©n bá»‹ dataset Ä‘áº§y Ä‘á»§
- [ ] CÃ i Ä‘áº·t mÃ´i trÆ°á»ng
- [ ] Cáº¥u hÃ¬nh hyperparameters
- [ ] Cháº¡y training
- [ ] ÄÃ¡nh giÃ¡ model
- [ ] Tá»‘i Æ°u hÃ³a náº¿u cáº§n
- [ ] Test integration
- [ ] Deploy

## ğŸ¯ Káº¿t quáº£ mong Ä‘á»£i

Sau khi fine-tuning, model sáº½ cÃ³ kháº£ nÄƒng:

- Táº¡o cÃ¢u chuyá»‡n tá»± nhiÃªn tá»« thÃ´ng tin tháº» bÃ i
- Duy trÃ¬ phong cÃ¡ch nháº¥t quÃ¡n
- TÃ­ch há»£p hiá»‡u á»©ng game mÆ°á»£t mÃ 
- Táº¡o ná»™i dung giÃ¡o dá»¥c cháº¥t lÆ°á»£ng cao
